---
title: "ECB Project"
output:
  pdf_document: 
    toc: true
    fig_caption: yes
    toc_depth: 3
    latex_engine: lualatex
    keep_tex: false
header-includes:
  - \usepackage{amsmath}
  - \usepackage[backend=biber,style=authoryear]{biblatex}  # load biblatex
  - \addbibresource{citations.bib}  # BibTeX file
---
\newpage

# Preliminaries

## Setup
```{r setup, results=FALSE, warning=FALSE, message=FALSE, echo=TRUE}

# Clear memory 
rm(list=ls())

# Load here package to enable finding script loading other packages
require(here)

# Set directory
getwd()
setwd("...") 

# Load packages & helper functions
source(here("helpers/packages.R"))
source(here("helpers/auto_ARIMA_replic.R"))
source(here("helpers/stationarity_tests.R"))
source(here("helpers/roll_TR_plotter.R"))
source(here("helpers/pseudo_outofsample_tables.R"))
source(here("helpers/actual_forecast_helpers.R"))

# Api key for data
fredr_set_key("e0169694a62c1337f1969e3872605eca")

# Dates (to automatically get the latest data from API calls)
start_date <- "1999-01-01"
end_date <- Sys.Date()

# For replication 
set.seed(2025)

```

## Interactive Option Selection

* Use Hamilton Filter: 
  + TRUE: Selects Hamilton method for output gap estimation
  + FALSE: Selects Hodrick-Prescott method for output gap estimation

* Use Inflation Expectations:
  + TRUE: The models used for forecasting will use 12-month ahead inflation
          expectations from the ECB survey of professional forecasts (average).
  + FALSE: The models used for forecasting will use realised inflation
  
* Use Formula
  + Formula 1: Actual interest rate regressed on inflation and output gaps  
  + Formula 2: Shadow interest rate regressed on inflation and output gaps
  + Formula 3: Actual interest rate regressed on the one-quarter lag of the interest
                rate and on inflation and output gaps
  + Formula 4: Shadow interest rate regressed on the one-quarter lag of the shadow 
                interest rate and on inflation and output gaps
                
* Format:
  + html: For outputting in console or knitting to html
  + latex: For knitting to pdf

```{r options, results=FALSE, warning=FALSE, message=FALSE, echo=TRUE}

# Related to Analysis
USE_HAMILTON_FILTER <- TRUE
USE_INFLATION_EXPECTATIONS <- FALSE
USE_FORMULA <- "Formula 3"

# Related to Document Output
format <- "html"
#format <- "latex" 
save_figures <- FALSE #note: format has to be set to latex

```

```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```

# Data

## Sources \& Explanations

* FRED Data Source: European Central Bank, ECB Deposit Facility Rate for Euro 
                    Area [ECBDFR], retrieved from FRED, Federal Reserve Bank of 
                    St. Louis; https://fred.stlouisfed.org/series/ECBDFR. 
                    The data is the Deposit Facility and is directly reprinted 
                    from the ECB. It's in Percent and not seasonally adjusted. 
                    The ECB Monetary Policy is steered through this rate

* Shadow Interest Rate: The Shadow rate was developed by 
                        \cite{wuTimeVaryingLowerBound2017} and does quantify 
                        a hypothetical removal of the ZLB. The rate does not 
                        track 1:1 on the deposit facility in the data before 
                        the ZLB, instead being closer to the refinancing rate. 
                        (maybe need to adjust)

* GDP: The GDP Data is quarterly real GDP in 2010 Euros in million retrieved from FRED 
       via Eurostat. The data is seasonally adjusted.

* Potential GDP: Either estimated with the Hodrick-Prescott filter or the Hamilton filter, based on the formula below:
    $$
      \begin{aligned}
      \text{HP Filter:} \quad & \min_{\tau} \left( \sum_{t=1}^{T} (y_t - \tau_t)^2 + \lambda \sum_{t=2}^{T-1} \left[ (\tau_{t+1} - \tau_t) - (\tau_t - \tau_{t-1}) \right]^2 \right) \\
      \text{Hamilton:} \quad & y_t = \beta_0 + \sum_{j=1}^{p} \beta_j y_{t-h-j+1} + v_t \quad \text{(where } v_t \text{ is the cycle)}
      \end{aligned}
    $$

* Inflation:
  + Realised: The Inflation data is from Eurostat and measures HICP monthly data (annual rate of change). It's the index that the ECB uses for Inflation and is not seasonally adjusted, but the 
    fact that it represents the year-on-year change in prices implies there is no seasonality.
  + Expectations: Expected Inflation is the ECB's survey of professional 
    forecasters. It forecasts the HICP 12 months in advance.


## Loading \& Preparation Data 
```{r data_main, message=FALSE, warning=FALSE}

# --- 1. ECB Deposit Facility Rate & Shadow Rate ---
ecb_rate_daily <- fredr(series_id = "ECBDFR", observation_start = as.Date(start_date))
ecb_rate_q <- ecb_rate_daily %>%
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarise(rate = last(value)) %>%
  mutate(date = as.Date(quarter))
# Wu-Xia Shadow Rate 
shadow_rate_daily = as.data.frame(readMat("data/shadowrate_ECB.mat")) 
colnames(shadow_rate_daily) <- c("DATE", "shadowrate")
shadow_rate_daily$DATE <- as.Date(paste0(shadow_rate_daily$DATE, "01"), format="%Y%m%d")
shadow_rate_daily$quarter <- as.yearqtr(as.Date(shadow_rate_daily$DATE))
shadow_rate_daily$month <- as.yearmon(as.Date(shadow_rate_daily$DATE))
quarterly_shadow = aggregate(shadowrate ~ quarter, data=shadow_rate_daily, FUN=mean, na.rm=T)
monthly_shadow = aggregate(shadowrate ~ month, data=shadow_rate_daily, FUN=mean, na.rm=T)

# --- 2. HICP Inflation (Euro Area) ---
inflation_data <- get_eurostat("prc_hicp_manr", filters = list(geo = "EA", coicop = "CP00"), type = "label")
inflation_q <- inflation_data %>%
  filter(time >= start_date) %>%
  dplyr::select(date = time, inflation = values) %>%
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarise(inflation = mean(inflation, na.rm = TRUE)) %>%
  mutate(date = as.Date(quarter))

#inflation expectations
inflation_exp <- rdb(ids = "ECB/SPF/M.U2.HICP.POINT.P12M.Q.AVG")
#inflation_exp <- rdb(ids = "ECB/SPF/M.U2.HICP.POINT.P24M.Q.AVG")
inflation_exp_q <- inflation_exp %>%
  mutate(quarter = as.yearqtr(period)) %>%
  group_by(quarter) %>%
  summarise(exp_inflation = last(original_value)) %>%
  mutate(date = as.Date(quarter))

#P12M : 12-month ahead forecasts
inflation_q$exp_inflation = c(rep(NA,3),as.numeric(inflation_exp_q$exp_inflation),NA)
#P24M : 24-month ahead forecasts
#inflation_q$exp_inflation = c(rep(NA,7),as.numeric(inflation_exp_q$exp_inflation[1:101]))

# --- 3. Real GDP and Estimated Output Gap ---
# a) Real GDP for the Euro Area. The series ID is CLVMNACSCAB1GQE_A.
gdp_q <- fredr(
  series_id = "CLVMEURSCAB1GQEA19",
  observation_start = as.Date(start_date)) %>%
  mutate(quarter = as.yearqtr(date)) %>%
  dplyr::select(quarter, real_gdp = value) %>%
  mutate(log_real_gdp = log(real_gdp)) 

# b) Estimate Potential GDP (the trend) using the HP Filter on the log of real GDP.
# The lambda value of 1600 is standard for quarterly data.
hp_gdp <- hpfilter(gdp_q$log_real_gdp, freq = 1600)
gdp_q$potential_gdp_log <- as.numeric(hp_gdp$trend)
ham_gdp_cycle <- filter_hamilton(gdp_q$log_real_gdp, p = 4, horizon = 8)
gdp_q$potential_gdp_log_ham <- gdp_q$log_real_gdp - ham_gdp_cycle

# Combine all data into a single data frame
data <- ecb_rate_q %>%
  dplyr::select(quarter, rate) %>%
  left_join(inflation_q, by = "quarter") %>%
  left_join(gdp_q, by = "quarter") %>%
  left_join(quarterly_shadow, by = "quarter") 

# Create model variables
data <- data %>%
  mutate(
    realised_inflation_gap = inflation - 2.0,
    exp_inflation_gap = exp_inflation -2.0,
    output_gap_hp = 100 * (log_real_gdp - potential_gdp_log),
    output_gap_ham = 100 * (log_real_gdp - potential_gdp_log_ham),
    rate_lag = lag(rate, 1),
    shadowrate = case_when(
      quarter < "2012 Q3" | quarter >= "2022 Q3" ~ rate,
      TRUE ~ shadowrate),
    shadowrate_lag = lag(shadowrate, 1))

# Remove last row since no output data
data = subset(data, quarter < "2025 Q3")

# Clean environment
rm(gdp_q, hp_gdp, ecb_rate_daily, ecb_rate_q, inflation_data, inflation_q,
   inflation_exp, inflation_exp_q, monthly_shadow, quarterly_shadow, shadow_rate_daily)

```

## Options Configuration
```{r data_config, message=TRUE, warning=FALSE, results='asis'}

# Choices in setup chunk

# --------- 1. Filter selection for output gap estimation ----------

# TRUE  = Use Hamilton Filter (newer, arguably more robust)
# FALSE = Use HP Filter (classic approach)

# Applying selection
if (USE_HAMILTON_FILTER) {
    data$output_gap <- data$output_gap_ham
  cat("* CONFIGURATION: Using Hamilton Filter for output gap estimation.")
} else {
  data$output_gap <- data$output_gap_hp 
  cat("* CONFIGURATION: Using HP Filter for output gap estimation.") }

# --------- 2. Inflation expectations choice ----------

# TRUE  = Use inflation expectations from ECB survey of professional forecasts
# FALSE = Use realised inflation

# Applying selection
if (USE_INFLATION_EXPECTATIONS) {
    data$inflation_gap <- data$exp_inflation_gap
  cat("* CONFIGURATION: Using inflation expectations in Taylor Rule forecasting.")
} else {
  data$inflation_gap <- data$realised_inflation_gap 
  cat("* CONFIGURATION: Using realised inflation in Taylor Rule forecasting.") }

```

## Raw Data Plots
```{r data_plots, message=FALSE, warning=FALSE}

# Data must be in long format for a faceted plot
plot_data <- data %>%
  pivot_longer(cols = c(rate, inflation, exp_inflation, output_gap_hp, output_gap_ham),
               names_to = "series",
               values_to = "value") %>%
  mutate(series = factor(series, 
    levels = c("rate", "inflation", "exp_inflation", "output_gap_hp", "output_gap_ham"),
    labels = c("Deposit Rate", "(Realised) Inflation", "(Expected) Inflation", "Output Gap (HP)", "Output Gap (Hamilton)")))

raw_plot = ggplot(plot_data, aes(x = date, y = value)) +
  geom_line(aes(color = series), linewidth = 1) +
  facet_wrap(~ series, scales = "free_y", ncol = 2) +
  scale_x_date(date_breaks = "5 years", date_labels = "%Y") +
  labs(title = "Raw Data Plots", subtitle = "Y axis in %", x = "", y = "") +
  theme_minimal() + theme(
     plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
     plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
     axis.title = element_text(size = 12),
     axis.text = element_text(size = 10),
     legend.position = "none",
     panel.grid.major.x = element_line(linewidth = 0.525, color = "grey83"))
raw_plot
if (save_figures) {
  ggsave(filename = "raw_data_plot.png", path = "figures/", plot = raw_plot)}
rm(plot_data)

```

## Data Properties
```{r data_prop, message=FALSE, warning=FALSE}

# -----  Run Tests -----

# Use helper to run stationarity checks for our main variables
test_rate <- check_stationarity(data$rate, "Interest Rate")
test_inflation <- check_stationarity(data$inflation, "Inflation")
test_output_gap <- check_stationarity(na.omit(data$output_gap), "Output Gap")

# Given results for rate and inflation, run tests on 1st diffs
test_rate_diff <- check_stationarity(diff(data$rate), "Interest Rate (1st Diff)")
test_inflation_diff <- check_stationarity(diff(data$inflation), "Inflation (1st Diff)")

# Since rate and inflation are I(1), run a cointegration test
coint_test = check_coint(data$rate, data$inflation, 
            var_name1 = "Interest Rate", var_name2 = "Inflation")


# ----- Print Results -----

# Combine stationarity results 
all_stationarity_results <- rbind(test_rate, 
                                  test_inflation, 
                                  test_output_gap,
                                  test_rate_diff,
                                  test_inflation_diff)

# Tables
stat_table <- kable(all_stationarity_results, digits = 4, format = format, booktabs = TRUE, 
      caption = "Summary of Stationarity Tests (ADF \\& KPSS)") %>%
      kable_styling(latex_options = "scale_down",
      position = "center") %>%
        column_spec(1, border_right = TRUE) %>%
        column_spec(4, width = "6cm")  
stat_table
if (save_figures) {
  save_kable(stat_table, "figures/staionarity_results.tex")}

coint_table <- kable(coint_test, digits = 4, format = format, booktabs = TRUE,
      caption = "Cointegration Test Results") %>%
      kable_styling(latex_options = "scale_down",
      position = "center") %>%
        column_spec(1, border_right = TRUE) 
coint_table
if (save_figures) {
  save_kable(coint_table, "figures/cointegration_results.tex")}

```


```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```

# Taylor Rule Estimation

## Without Lags
$$
\begin{aligned}
  i_t &= \pi^* + \beta(\pi_t-\pi^*) + \gamma(y_t-\bar{y_t})  \\
\end{aligned}
$$

```{r Taylor Rule simple w/o lag}

TR <- lm(rate ~ realised_inflation_gap + output_gap, data = data)
TRsr <- lm(shadowrate ~ realised_inflation_gap + output_gap, data = data)

table1 <- export_summs(TR, TRsr, vcov = sandwich::NeweyWest, 
          model.names = c("TR", "TR w/ SR"), digits = 4)
huxtable::caption(table1) <- "No Lag, No Expectations"
if (save_figures) {
  huxtable::quick_latex(table1, file = "figures/TR_no_lag_no_expectations.tex")}
table1

```

```{r Taylor Rule simple w/o lag but with inflatione expectations}

TR_e <- lm(rate ~ exp_inflation_gap + output_gap, data = data)
TRsr_e <- lm(shadowrate ~ exp_inflation_gap + output_gap, data = data)
TR_ie <- lm(rate ~ realised_inflation_gap + exp_inflation_gap + output_gap, data = data)
TRsr_ie <- lm(shadowrate ~ realised_inflation_gap + exp_inflation_gap + output_gap, data = data)

table2 <- export_summs(TR_e, TRsr_e, TR_ie, TRsr_ie, vcov = sandwich::NeweyWest, 
          model.names = c("TR", "TR w/ SR", "TR", "TR w/ SR"), digits = 4)
huxtable::caption(table2) <- "No Lag, with Inflation Expectations"
if (save_figures) {
  huxtable::quick_latex(table2, file = "figures/TR_no_lag_w_expectations.tex")}
table2

```


## Lagged Models 
$$
\begin{aligned}
     i_t &= \pi^* + \phi i_{t-1} + \beta (\pi_t-\pi^*) + \gamma(y_t-\bar{y_t}) 
\end{aligned}
$$

```{r Taylor Rule simple w/ lag}

lTR <- lm(rate ~ rate_lag + realised_inflation_gap + output_gap, data = data)
lTRsr <- lm(shadowrate ~ shadowrate_lag + realised_inflation_gap + output_gap, data = data)

table3 <- export_summs(lTR, lTRsr, vcov = sandwich::NeweyWest, 
          model.names = c("TR", "TR w/ SR"), digits = 4)
huxtable::caption(table3) <- "Interest Rate Lag, No Expectations"
if (save_figures) {
  huxtable::quick_latex(table3, file = "figures/TR_w_lag_no_expectations.tex")}
table3

```

```{r Taylor Rule simple w/ lag and inflation expectations}

lTR_e <- lm(rate ~ rate_lag + exp_inflation_gap + output_gap, data = data)
lTRsr_e <- lm(shadowrate ~ shadowrate_lag + exp_inflation_gap + output_gap, data = data)
lTR_ie <- lm(rate ~ rate_lag + realised_inflation_gap + exp_inflation_gap + output_gap, data = data)
lTRsr_ie <- lm(shadowrate ~ shadowrate_lag + realised_inflation_gap + exp_inflation_gap + output_gap, data = data) 

table4 <- export_summs(lTR_e, lTRsr_e, lTR_ie,lTRsr_ie, vcov = sandwich::NeweyWest, 
          model.names = c("TR", "TR w/ SR", "TR", "TR w/ SR"), digits = 4)
huxtable::caption(table4) <- "Interest Rate Lag, with Inflation Expectations"
if (save_figures) {
  huxtable::quick_latex(table4, file = "figures/TR_w_lag_w_expectations.tex")}
table4

```

## Checking for structural breaks
```{r chowtest}

# Formula to test for breaks
break_formula = rate ~ rate_lag + inflation_gap + output_gap

# Suspected break: Start of ZLB in 2012 Q3
breakpoint1_obs = 55 #R = 55 in evaluation chunk
breakpoint1_date <- data$quarter[breakpoint1_obs]

# Suspected break: Covid
breakpoint2_obs = 85 #R = 85 in evaluation chunk
breakpoint2_date <- data$quarter[breakpoint2_obs]

# Chow test (rejecting the null means there are structural breaks)
chow_test1 <- sctest(break_formula, type = "Chow", point = breakpoint1_obs, data = data)
chow_test2 <- sctest(break_formula, type = "Chow", point = breakpoint2_obs, data = data)

# Table with Chow results (for suspected breaks)
chow_df <- data.frame(
  Event = c("ZLB Start", "COVID-19 Start"),
  Date = c(as.character(breakpoint1_date), as.character(breakpoint2_date)),
  `p-value` = c(chow_test1$p.value, chow_test2$p.value),check.names = FALSE)

chow_table <- kable(chow_df, digits = 4, format = format, booktabs = TRUE,
      caption = "Chow tests for suspected structural breaks") %>%
      kable_styling(latex_options = "scale_down",
      position = "center") %>%
        column_spec(1, border_right = TRUE) 
chow_table
if (save_figures) {
  save_kable(chow_table, "figures/chow_breaks.tex")}

```

```{r bai-perron test}

# Estimate Bai-Perron test & output results
BP_test = breakpoints(break_formula, data = data)
BP_test_res = summary(BP_test)

# Optimal values (modular!)
bic_values <- BP_test_res$RSS[2, ]
optimal_m <- as.numeric(names(bic_values)[which.min(bic_values)])

# Make a table out of results (also modular!)
if (optimal_m == 0) {
  bp_df <- data.frame(
    `Detected Breaks` = "No structural breaks detected",
    check.names = FALSE)
  } else {
  break_obs <- na.omit(BP_test_res$breakpoints[optimal_m, ])
  detected_dates <- data$quarter[break_obs]
  bp_df <- data.frame(
    `Detected Breaks` = as.character(detected_dates),
    check.names = FALSE) }

bp_table <- kable(bp_df, format = format, booktabs=TRUE,
  caption = "Bai-Perron test for multiple breaks") %>%
  kable_styling(latex_options = "scale_down",
      position = "center") 
bp_table
if (save_figures) {
  save_kable(bp_table, "figures/bp_breaks.tex")}
# Note: breaks_obs shows in which row the BP breaks are 

```






## Rolling Estimation (for structural breaks)
```{r rolling TR, message=FALSE, warning=FALSE}

# Set rolling window (in quarters) & Looping Parameter
W = 30
L = nrow(data) - W + 1
formula = rate ~ rate_lag + inflation_gap + output_gap

# Preparation of result data, dates, var names, and confidence intervals
var_names <- attr(terms(formula), "term.labels")
window_end_dates <- data$quarter[W:nrow(data)] # First window [1:W] ends at data$date[W]
TR_roll <- data.frame(date = window_end_dates)
TR_roll[var_names] <- NA
lower_col_names <- paste0(var_names, "_lower")
upper_col_names <- paste0(var_names, "_upper")

# Looped estimation of TR, outputs coefficients and CIs
for (l in 1:L) {
  # 1. Define splits (with rolling scheme)
  rolled_data <- data[l:(W + l - 1), ]
  
  # 2. Estimate TR on split data, using whatever formula is desired
  TR_estimate <- lm(formula, data = rolled_data)

  # 3. Pull out coefficients & compute confidence intervals
  all_coefs <- coef(TR_estimate)
  all_cis <- confint(TR_estimate)
  
  TR_roll[l, var_names] <- all_coefs[var_names] 
  TR_roll[l, lower_col_names] <- all_cis[var_names, 1]
  TR_roll[l, upper_col_names] <- all_cis[var_names, 2]   }

# Plotting (note: this part is not modular, obviously)
plot_rolling_coefs(TR_roll, "rate_lag", var_name_title="Rate Lag")
plot_rolling_coefs(TR_roll, "inflation_gap", var_name_title="Inflation (Gap)")
plot_rolling_coefs(TR_roll, "output_gap", var_name_title="Output Gap")

```


```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```


# Forecasting Model Evaluation

## Estimation

Pseudo-out of sample rolling estimation scheme of direct forecasts for all
Taylor Rule formulas and a benchmark ARIMA specification. 

```{r forecast-model-main, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# Parameters
R = 85 # Chow: Structural breaks at R=55 and R=85 
cat("Evaluation sample starts after ",as.character(data$quarter[R]),".",sep="")
P = nrow(data) - R #but will effectively be: P = T-h-R
H = 10 #number of different horizons (takes 10 to go until 2027 Q4)

#-------------------------------------------------------------------
# 1. DEFINE THE TAYLOR RULE (TR) MODEL FORMULAS
#-------------------------------------------------------------------

# TR specifications (using either current inflation or inflation expectations
#                    according to configuration, same with HP vs Hamilton)
formula_1 <- rate ~ inflation_gap + output_gap
formula_2 <- shadowrate ~ inflation_gap + output_gap
formula_3 <- rate ~ rate_lag + inflation_gap + output_gap
formula_4 <- shadowrate ~ shadowrate_lag + inflation_gap + output_gap

#-------------------------------------------------------------------
# 2. PRE-ALLOCATE STORAGE FOR ALL RESULTS
#-------------------------------------------------------------------

# We need 4 lists for the TR models, 1 list for the shared benchmark
init_storage_list <- function(H, P) {
  storage <- vector("list", length = H)
  for (h in 1:H) {
    storage[[h]] <- rep(NA_real_, P)}
  return(storage)}

# Storage for realised values 
Actuals <- init_storage_list(H, P)

# Storage for Forecasts 
F_TR_1 <- init_storage_list(H, P) # Model 1: shadowrate, no lag
F_TR_2 <- init_storage_list(H, P) # Model 2: rate, no lag
F_TR_3 <- init_storage_list(H, P) # Model 3: shadowrate, with lag
F_TR_4 <- init_storage_list(H, P) # Model 4: rate, with lag
F_BM   <- init_storage_list(H, P) # Benchmark: ARIMA

# Storage for Forecast Errors 
FE_TR_1 <- init_storage_list(H, P) # Model 1: shadowrate, no lag
FE_TR_2 <- init_storage_list(H, P) # Model 2: rate, no lag
FE_TR_3 <- init_storage_list(H, P) # Model 3: shadowrate, with lag
FE_TR_4 <- init_storage_list(H, P) # Model 4: rate, with lag
FE_BM   <- init_storage_list(H, P) # Benchmark: ARIMA


#-------------------------------------------------------------------
# 3. SETUP & RUN THE PARALLEL BACKTESTING LOOP
#-------------------------------------------------------------------
num_cores <- detectCores() - 1 
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# .export sends read-only objects to each core
# .packages loads libraries on each core
worker_results <- foreach(
  p = P:1, 
  .packages = c("forecast", "stats", "dplyr"),
  .export = c("data", "H", "formula_1", "formula_2", "formula_3", "formula_4")
) %dopar% {
  
  # 1. Define splits (with rolling scheme)
  training <- data[(1 + nrow(data) - R - p):(nrow(data) - p), ]
  testing <- data[(nrow(data) - (p - 1)):nrow(data), ]
  
  # --- 2. Fit common models only once --- 
  # note: d=1 for interest and inflation as non-stationary
  inflation_arma <- my.auto.arima(training$inflation_gap, max.p=4, max.q=4, d=1)
  outputgap_arma <- my.auto.arima(training$output_gap, max.p=4, max.q=4, d=0)
  interest_arma <- my.auto.arima(training$rate, max.p=4, max.q=4, d=1) # Benchmark
  
  # --- 3. Get common forecasts only once (all H horizons) ---
  inflation_forecasts <- my.forecast(inflation_arma, h = H)
  outputgap_forecasts <- my.forecast(outputgap_arma, h = H)
  BMpredicted_rates <- my.forecast(interest_arma, h = H)
  
  # --- 4. Fit the 4 TR models ---
  TR_model_1 <- lm(formula_1, data = training)
  TR_model_2 <- lm(formula_2, data = training)
  TR_model_3 <- lm(formula_3, data = training)
  TR_model_4 <- lm(formula_4, data = training)
  
  # --- 5. Build forecast input data & get forecasts for non-lagged models ---
  # These are direct forecasts
  new_data_base <- data.frame(
    inflation_gap = inflation_forecasts,
    output_gap = outputgap_forecasts)
    
  TR_preds_1 <- round(pmax(predict(TR_model_1, new_data_base), min(data$rate)) / 0.25) * 0.25
  TR_preds_2 <- round(pmax(predict(TR_model_2, new_data_base), min(data$rate)) / 0.25) * 0.25
  BM_preds <- round(pmax(BMpredicted_rates, min(data$rate)) / 0.25) * 0.25
  
  # --- 6. Get forecasts for lagged models via iteration ---
  # We must loop 1 step at a time, feeding forecasts back in.   
  
  # a) Pre-allocate storage for H forecasts
  TR_preds_3 <- numeric(H)
  TR_preds_4 <- numeric(H)
  
  # b) Get the last known lag from the training set (lag for h=1 forecast)
  current_rate_lag  <- last(training$rate)
  current_shadowrate_lag <- last(training$shadowrate)

  # Loop for iterative forecasting 
  for (h in 1:H) {
    # --- Prepare dataset for predictions ---
    new_data_3_h <- data.frame(
      inflation_gap = inflation_forecasts[h],
      output_gap = outputgap_forecasts[h],
      rate_lag = current_rate_lag)
    new_data_4_h <- data.frame(
      inflation_gap = inflation_forecasts[h],
      output_gap = outputgap_forecasts[h],
      shadowrate_lag = current_shadowrate_lag )
    
    # Get the forecast values (keep for lag, and then round for actual prediction)
    pred_3_h <- predict(TR_model_3, new_data_3_h)
    TR_preds_3[h] <- round(pmax(pred_3_h, min(data$rate)) / 0.25) * 0.25
    pred_4_h <- predict(TR_model_4, new_data_4_h)
    TR_preds_4[h] <- round(pmax(pred_4_h, min(data$rate)) / 0.25) * 0.25
    
    # Update lag for h+1 
    current_shadowrate_lag <- pred_4_h
    current_rate_lag <- pred_3_h }

  # --- 7. Get actual values in evaluation sample  ---
  actual_rates <- testing$rate[1:H]
  
  # --- 8. Return all FORECASTS and ACTUALS from the worker ---
  list(F_TR_FORMULA_1 = TR_preds_1,
       F_TR_FORMULA_2 = TR_preds_2,
       F_TR_FORMULA_3 = TR_preds_3,
       F_TR_FORMULA_4 = TR_preds_4,
       F_BM  = BM_preds,
       actuals = actual_rates) }
    
# --- Stop the Cluster ---
stopCluster(cl)
rm(cl)

#-------------------------------------------------------------------
# 4. UNPACK PARALLEL RESULTS INTO STORAGE LISTS
#-------------------------------------------------------------------

# 'worker_results' is a list of P lists. We need to re-organize it.
for (i in 1:P) {
  # i=1 corresponds to p=P, i=2 to p=P-1, ... i=P to p=1
  # This 'storage_index' matches the loop order
  storage_index <- i 
  p_results <- worker_results[[i]]
  
  for (h in 1:H) {
    # Get the raw values for this h
    actual_val <- p_results$actuals[h]
    f_tr1_val  <- p_results$F_TR_FORMULA_1[h]
    f_tr2_val  <- p_results$F_TR_FORMULA_2[h]
    f_tr3_val  <- p_results$F_TR_FORMULA_3[h]
    f_tr4_val  <- p_results$F_TR_FORMULA_4[h]
    f_bm_val   <- p_results$F_BM[h]
    
    # Store Actuals (for MZ)
    Actuals[[h]][storage_index] <- actual_val
    
    # Store Forecasts (for MZ)
    F_TR_1[[h]][storage_index] <- f_tr1_val
    F_TR_2[[h]][storage_index] <- f_tr2_val
    F_TR_3[[h]][storage_index] <- f_tr3_val
    F_TR_4[[h]][storage_index] <- f_tr4_val
    F_BM[[h]][storage_index]   <- f_bm_val
    
    # Calculate and Store Errors (for MSFE/DM)
    FE_TR_1[[h]][storage_index] <- f_tr1_val - actual_val
    FE_TR_2[[h]][storage_index] <- f_tr2_val - actual_val
    FE_TR_3[[h]][storage_index] <- f_tr3_val - actual_val
    FE_TR_4[[h]][storage_index] <- f_tr4_val - actual_val
    FE_BM[[h]][storage_index]   <- f_bm_val  - actual_val } }

#-------------------------------------------------------------------
# 5. RENDER RESULTS MORE INTUITIVE FOR FURTHER ANALYSIS
#-------------------------------------------------------------------

# Convert the forecast lists (F_TR_x) into single dataframes
forecast_to_df <- function(forecast_list, period) {
  # Convert each element to numeric (benchmark is ts object, which is bad)
  numeric_list <- lapply(forecast_list, function(x) as.numeric(x)) #just make each list inside numeric
  df <- as.data.frame(numeric_list)
  # Add period and horizon
  df$period <- period #first list is all horizon 1 forecasts, gives this to all observations
  df$horizon <- 1:nrow(df) #counts rows and gives each the horizon corresponding to it
  df}

# Apply to all forecasting models
eval_all_models <- do.call(rbind, lapply(seq_along(worker_results), function(i) {
  forecast_to_df(worker_results[[i]], period = i) }))
eval_all_models[] <- lapply(eval_all_models, function(x) as.numeric(x))

```

## Spaghetti Plots
```{r Spaghetti Plot, message=FALSE, warning=FALSE}

# Select the model to plot based on initial option
if (USE_FORMULA == "Formula 1") {
  model <- "F_TR_FORMULA_1"
  model_formula <- formula_1
  model_name <- "Taylor Rule Formula 1"
} else if (USE_FORMULA == "Formula 2") {
  model <- "F_TR_FORMULA_2"
  model_formula <- formula_2
  model_name <- "Taylor Rule Formula 2"
} else if (USE_FORMULA == "Formula 3") {
  model <- "F_TR_FORMULA_3"
  model_formula <- formula_3
  model_name <- "Taylor Rule Formula 3"
} else if (USE_FORMULA == "Formula 4") {
  model <- "F_TR_FORMULA_4"
  model_formula <- formula_4
  model_name <- "Taylor Rule Formula 4" }

# period = date the forecast was made
# date_of_forecast = the future date we are predicting
eval_all_models$date_of_forecast <- eval_all_models$period + eval_all_models$horizon 

# Spaghetti plot with color per period
spag_plot1 <- ggplot(eval_all_models, aes(x = date_of_forecast, y = .data[[model]], group = period, color = factor(period))) +
  geom_line(alpha = 0.7) +    # forecast lines
  geom_point(shape = 2, alpha = 0.7) +
  # Actuals as black baseline
  geom_line(aes(y = actuals), color = "black", size = 1) +
  geom_point(aes(y = actuals), color = "black", shape = 4, alpha = 0.5) +
  labs(title = "Evaluation Sample Forecasts vs Realised Values",
       x = "Period",
       y = "Interest Rate",
       color = "Forecast Origin",
    subtitle = paste("For model based on:", model_name)) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
      plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10))
spag_plot1
if (save_figures) {
  ggsave(filename = "spaghetti_plot.png", path = "figures/", plot=spag_plot1)}


```

## Forecast Errors 

### Plots of FE
```{r Plot of Forecast Errors, warning=FALSE, message=FALSE}

# Compute forecast error
eval_all_models$forecast_error <- eval_all_models[[model]] - eval_all_models$actuals

#compute date_of_forecast for x-axis
eval_all_models$date_of_forecast <- eval_all_models$period + eval_all_models$horizon

spag_plot2 <- ggplot(eval_all_models, aes(x = date_of_forecast, group = period)) +
  # Forecast error lines
  geom_line(aes(y = forecast_error), color = "blue", alpha = 0.5) +
  geom_point(aes(y = forecast_error), color = "blue", alpha = 0.5, shape = 4) +
  
  # Actual rates line
  geom_line(aes(y = actuals), color = "black", size = 1) +
  geom_point(aes(y = actuals), color = "black", shape = 4, alpha = 0.5) +
  labs(title = "Evaluation Sample Forecast Errors",
       x = "Below Zero = forecast was too low ; Above Zero = forecast too high",
       y = "Interest Rate and Forecast Error (deviation from interest rate)",
    subtitle = paste("For model based on:", model_name))  +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
      plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10))
spag_plot2
if (save_figures) {
  ggsave(filename = "spaghetti_errors_plot.png", path = "figures/", plot=spag_plot2)}

```

### Density of FE
```{r Density of Forecast Errors}

# Version 1: Non-Adjusted Scales
plot_facet1 <- ggplot(eval_all_models %>% filter(horizon <= H), aes(x = forecast_error)) +
  geom_density(fill = "#3498db", color = "#2980b9", alpha = 0.6) +
  facet_wrap(~horizon, ncol = 4, labeller = label_both, scales = "free") +
  labs(title = "Density of Forecast Errors by Horizon (Non-Adjusted Scale)",
       subtitle = paste("For model based on:", model_name),
       x = "Forecast Error",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
        plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        strip.background = element_rect(fill = "#ecf0f1", color = NA), 
        strip.text = element_text(face = "bold"))
print(plot_facet1)
if (save_figures) {
  ggsave(filename = "density_plot.png", path = "figures/", plot = plot_facet1)}

# Version 2: Adjusted scales
plot_facet2 <- ggplot(eval_all_models %>% filter(horizon <= H), aes(x = forecast_error)) +
  geom_density(fill = "#3498db", color = "#2980b9", alpha = 0.6) +
  facet_wrap(~horizon, ncol = 4, labeller = label_both) +
  labs(title = "Density of Forecast Errors by Horizon (Adjusted Scale)",
       subtitle = paste("For model based on:", model_name),
       x = "Forecast Error",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
        plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        strip.background = element_rect(fill = "#ecf0f1", color = NA), 
        strip.text = element_text(face = "bold"))
print(plot_facet2)
if (save_figures) {
  ggsave(filename = "density_scaled_plot.png", path = "figures/", plot = plot_facet2)}

# Version 3: "Ridges"
plot_ridge <- ggplot(eval_all_models %>% filter(horizon <= H),
                       aes(x = forecast_error, y = as.factor(horizon), fill = stat(x))) +
    geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
    scale_fill_viridis_c(name = "Error", option = "C") +
    labs(title = "Density of Forecast Errors by Horizon",
         subtitle = paste("For model based on:", model_name),
         x = "Forecast Error",
         y = "Forecast Horizon") +
    theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
      plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10))
print(plot_ridge)
if (save_figures) {
  ggsave(filename = "densityridge_plot.png", path = "figures/", plot = plot_ridge)}

```

### Variance of FE
```{r Variance of Forecast Errors}

# This gives us the mean forecast error for the h step ahead forecast
var_by_horizon <- eval_all_models %>%
  group_by(horizon) %>%
  summarize(
    mean_fe = mean(forecast_error, na.rm=T),
    var_fe = sd(forecast_error, na.rm=T)^2, n = n() )

fe_var_plot <- ggplot(var_by_horizon, aes(x = horizon, y = var_fe)) +
  geom_line(color = "#2c3e50", size = 1) +
  geom_point(color = "#e74c3c", size = 3, alpha = 0.8) +
  theme_minimal(base_size = 14) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Variance of FE by Horizon",
    subtitle = paste("For model based on:", model_name),
    x = "Forecast Horizon",
    y = "Variance of FE") +
  theme(plot.title = element_text(face = "bold", size = 14, margin = margin(b=5)),
        plot.subtitle = element_text(size = 12, color = "grey30", margin = margin(b=10)),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        panel.grid.minor.x = element_blank() )
fe_var_plot
if (save_figures) {
  ggsave(filename = "FE_var_plot.png", path = "figures/", plot=fe_var_plot)}

```

### Horizon 1 Autocorrelation of FE
```{r Durbin Watson test, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#Durbin Watson tests for first autocorrelation at h=1

# Select Forecast Columns
formula_cols <- grep("^F_TR_FORMULA_", names(eval_all_models), value = TRUE)

# automatically set max horizon for loop
max_h <- max(eval_all_models$horizon)

# Loop for each model
for (e_model_name in formula_cols) {

  # Extract and regress errors
  all_forecast_errors <- eval_all_models[["actuals"]] - eval_all_models[[e_model_name]]
  h1_errors_vector <- all_forecast_errors[eval_all_models$horizon == 1]
  temp_model <- lm(h1_errors_vector ~ 1)

  #Durbin-Watson Test
  dw_test_result <- durbinWatsonTest(temp_model)

  # Print output
  cat(sprintf("\n--- Durbin-Watson Test for Model: %s (h=1) ---\n", e_model_name))
  print(dw_test_result)
  cat(sprintf("DW Statistic: %.4f\n", dw_test_result$statistic))}

```


### Overall Autocorrelation of FE
```{r Ljung Box test, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#Ljung Box Test, with Columns & max lags set in Durbin Watson code chunk

# loop through each TR
for (e_model_name in formula_cols) {
    
    cat(sprintf("--- Checking Errors for Model: %s ---\n", e_model_name))
    
    # Calculate the errors of the model
    all_errors <- eval_all_models[["actuals"]] - eval_all_models[[e_model_name]]
    
    # Loop over all forecast horizons to test for autocorrelation
    for (h in 1:max_h){
        
        # select errors for each horizon
        h_errors <- all_errors[eval_all_models$horizon == h]
            
        # max lag according to slides around sqrt T
        T_errors <- length(h_errors)
        max_lag <- 4 #round(sqrt(T_errors)) # use 4 or 5 chose 4 because 1 year has 4 quarters
        
        # Ljung-Box Test
        lb_test_result <- Box.test(h_errors, 
                                   lag = max_lag, 
                                   type = "Ljung-Box")
        
        #Print Results
        cat(sprintf("Horizon h=%d (N=%d, Lags=%d): Q=%.2f, p-value=%.3f\n", 
                    h, T_errors, max_lag, lb_test_result$statistic, lb_test_result$p.value))}
        cat("\n")}

```


## Absolute Performance: Efficiency \& Bias
```{r forecast-model-efficiency, echo=TRUE, message=TRUE, warning=FALSE, results='asis'}

# Call MZ-test helper function 4 times.
#  Note: These reports is wrapped in trycatch as it sometimes fails
#         If it does fail, simply decrease R in order to have more 
#          observations, removing potential multicolinearity.

# MZ Report 1: Actual Rate, No Lag
mz_report_1 <- tryCatch({
  generate_mincer_zarnowitz_report(
    F_model = F_TR_1,
    Actual_values = Actuals,
    H = H,
    model_caption = "Mincer-Zarnowitz Test: Actual Rate, No Lag",
    format = format)}, error = function(e) {
  message("Error generating MZ Report (Actual Rate, No Lag): ", e$message)
  message("Skipping this report and continuing...")
  return(NULL)})

# MZ Report 2: Shadow Rate, No Lag (
mz_report_2 <- tryCatch({
  generate_mincer_zarnowitz_report(
    F_model = F_TR_2,
    Actual_values = Actuals,
    H = H,
    model_caption = "Mincer-Zarnowitz Test: Shadow Rate, No Lag",
    format = format)}, error = function(e) {
  message("Error generating MZ Report (Shadow Rate, No Lag): ", e$message)
  message("Skipping this report and continuing...")
  return(NULL)})

# MZ Report 3: Actual Rate, with Lag
mz_report_3 <- tryCatch({
  generate_mincer_zarnowitz_report(
    F_model = F_TR_3,
    Actual_values = Actuals,
    H = H,
    model_caption = "Mincer-Zarnowitz Test: Actual Rate, with Lag",
    format = format)}, error = function(e) {
  message("Error generating MZ Report (Actual Rate, with Lag): ", e$message)
  message("Skipping this report and continuing...")
  return(NULL)})

# MZ Report 4: Shadow Rate, with Lag
mz_report_4 <- tryCatch({
  generate_mincer_zarnowitz_report(
    F_model = F_TR_4,
    Actual_values = Actuals,
    H = H,
    model_caption = "Mincer-Zarnowitz Test: Shadow Rate, with Lag",
    format = format)}, error = function(e) {
  message("Error generating MZ Report (Shadow Rate, with Lag): ", e$message)
  message("Skipping this report and continuing...")
  return(NULL)})

# MZ Report 5: Benchmark
mz_report_BM <- tryCatch({
  generate_mincer_zarnowitz_report(
    F_model = F_BM,
    Actual_values = Actuals,
    H = H,
    model_caption = "Mincer-Zarnowitz Test: Benchmark ARIMA",
    format = format)}, error = function(e) {
  message("Error generating MZ Report (Benchmark ARIMA): ", e$message)
  message("Skipping this report and continuing...")
  return(NULL)})

list(mz_report_1, mz_report_2, mz_report_3, mz_report_4, mz_report_BM)

```


## Relative Performance (against benchmark)
```{r forecast-model-perf, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# Call DM-test helper function 4 times.

# Report 1: Actual Rate, No Lag
report_1 <- generate_report_table(
  FE_TR_model = FE_TR_1,
  FE_BM_model = FE_BM,
  H = H,
  model_caption = "MSFE Comparison, Trained on Actual Rate, No Lag",
  format = format)

# Report 2: Shadow Rate, No Lag
report_2 <- generate_report_table(
  FE_TR_model = FE_TR_2,
  FE_BM_model = FE_BM,
  H = H,
  model_caption = "MSFE Comparison, Trained on Shadow Rate, No Lag",
  format = format)

# Report 3: Actual Rate, with Lag
report_3 <- generate_report_table(
  FE_TR_model = FE_TR_3,
  FE_BM_model = FE_BM,
  H = H,
  model_caption = "MSFE Comparison, Trained on Actual Rate, with Lag",
  format = format)

# Report 4: Shadow Rate, with Lag
report_4 <- generate_report_table(
  FE_TR_model = FE_TR_4,
  FE_BM_model = FE_BM,
  H = H,
  model_caption = "MSFE Comparison, Trained on Shadow Rate, with Lag",
  format = format)

list(report_1, report_2, report_3, report_4)

```

```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```



# Actual Forecast Model

## Forecasting
```{r forecast, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# Formula 4 seems to work best
our_predict <- function(data,formula,H){

  # --- 1. Fit inputs and benchmark models  ---
   inflation_arma <- my.auto.arima(data$inflation_gap, max.p=4, max.q=4, d=1, var_name="Inflation Gap")
   outputgap_arma <- my.auto.arima(data$output_gap, max.p=4, max.q=4, d=0, var_name="Output Gap")
   interest_arma <- my.auto.arima(data$rate, max.p=4, max.q=4, d=1, var_name="Interest Rate") # Benchmark
  
  # --- 2. Get forecasts of inputs (all H horizons) ---
   inflation_forecasts <- my.forecast(inflation_arma, h = H)
   outputgap_forecasts <- my.forecast(outputgap_arma, h = H)
   BMpredicted_rates <- my.forecast(interest_arma, h = H)
  
  # --- 3. Fit TR model
   TR_model <- lm(formula, data = data)
    
  # --- 4. Build forecast input data frame (iteratively for lags) ---
   
   # Allocate storage for full horizon
   TR_preds <- numeric(H)
   
   # Get last known lags (starting point for lagged models)
   current_shadowrate_lag <- last(data$shadowrate)
   current_rate_lag <- last(data$rate)
   
  for (h in 1:H) {
     new_data_h <- data.frame(
        inflation_gap = inflation_forecasts[h],
        output_gap = outputgap_forecasts[h],
        shadowrate_lag = current_shadowrate_lag,
        rate_lag = current_rate_lag)
     
     # Get forecasted values
     pred_h <- predict(TR_model, new_data_h)
     TR_preds[h] <- round(pmax(pred_h, min(data$rate)) / 0.25) * 0.25
     
     # Update the lag for h+1
     current_rate_lag <- pred_h }
     
   # --- 5. Compute forecast for BM ---
    BM_preds <- round(pmax(BMpredicted_rates, min(data$rate)) / 0.25) * 0.25 
    
  return(list(TR_Forecast = TR_preds, 
                BM_Forecast = BM_preds,
                Inflation_Forecast = inflation_forecasts + 2, #to add back target 
                OutputGap_Forecast = outputgap_forecasts,  
                inflation_gap_arma_coef = inflation_arma$coef,
                output_gap_arma_coef = outputgap_arma$coef,
                benchmark_arma_coef = interest_arma$coef,
                inflation_gap_arma_var = inflation_arma$sigma2,
                output_gap_arma_var = outputgap_arma$sigma2,
                benchmark_arma_var = interest_arma$sigma2))}

 
final_forecasts <- our_predict(data = data, formula = model_formula, H = H)
 
display_forecasts(final_forecasts, 
                  caption = paste("For model based on:", model_name),
                  format = format)

final_fc_plot1 <- plot_forecasts(final_forecasts)
final_fc_plot1
if (save_figures) {
  ggsave(filename = "actual_forecasts_plot.png", path = "figures/", plot=final_fc_plot1)}
 
```

## Prediction Intervals

```{r prediction_intervals, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# Preparing data with point forecasts and variance
prediction <- var_by_horizon
final_interval <- final_forecasts[c(1,2,3,4)]
prediction$sd_fe <- sqrt(prediction$var_fe)

# Computing confidence intervals 
final_interval$sd <- prediction$sd_fe
final_interval$upper_1_sd <- final_interval$sd + final_interval$TR_Forecast
final_interval$lower_1_sd <- final_interval$sd*(-1) + final_interval$TR_Forecast

final_interval$upper_2_sd <- final_interval$sd*2 + final_interval$TR_Forecast
final_interval$lower_2_sd <- final_interval$sd*2*(-1) + final_interval$TR_Forecast

final_interval <- as.data.frame(final_interval)

# Plotting our actual forecast including said intervals
final_fc_plot2 <- ggplot(final_interval, aes(x = seq_len(nrow(final_interval)), y = TR_Forecast, group = 1)) +
  
  # --- 1. Confidence/Prediction Intervals ---
  # 2-Standard Deviation Band (e.g., ~95% Interval for Normal Errors)
  geom_ribbon(aes(ymin = lower_2_sd, ymax = upper_2_sd), fill = "#b3cde3", alpha = 0.5) +
  # 1-Standard Deviation Band (e.g., ~68% Interval)
  geom_ribbon(aes(ymin = lower_1_sd, ymax = upper_1_sd), fill = "#8856a7", alpha = 0.3) +
  geom_line(color = "#4c78a8", size = 1.2, linetype = "solid") +
  geom_point(color = "#4c78a8", size = 3, shape = 21, fill = "white", stroke = 1) + 
  labs(title = "Monetary Policy Rate Forecast with Uncertainty Bands",
       subtitle = paste("Model: Taylor Rule with", model_name, "| Confidence Bands based on Standard Error"),
       caption = "Inner band: ±1 S.D. (approx. 68%); Outer band: ±2 S.D. (approx. 95%)",
       x = "Forecast Horizon (h-steps ahead)", y = "Interest Rate (%)") +
  theme_light() +
  theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5, margin = margin(b=7)),
        plot.subtitle = element_text(size = 12, color = "#555555", hjust = 0.5, margin = margin(b=5)),
        plot.caption = element_text(size = 9, color = "grey50", hjust = 0),
        axis.title.x = element_text(size = 12, margin = margin(t=10)),
        axis.title.y = element_text(size = 12, margin = margin(r=10)),
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 0, hjust = 0.5) )
final_fc_plot2

```



```{r temporary prediction interval}

# ce graph c est le old ou le new? 

# Get last observation
H <- length(final_interval$TR_Forecast)
forecast_quarters_yearqtr <- seq(from = last(data$quarter) + 0.25, 
                                 by = 0.25, 
                                 length.out = H)
 
# Get numeric values
forecast_quarters_numeric <- as.numeric(forecast_quarters_yearqtr)
 
# Turn into characters
forecast_quarters_labels <- as.character(forecast_quarters_yearqtr)

# Plot the forecast plot
final_fc_plot2 <- ggplot(final_interval, aes(x = forecast_quarters_numeric, y = TR_Forecast, group = 1)) +

  #Prediction INtervall
  geom_ribbon(aes(ymin = lower_2_sd, ymax = upper_2_sd), fill = "#C0C0C0", alpha = 0.5) +
  geom_ribbon(aes(ymin = lower_1_sd, ymax = upper_1_sd), fill = "#A9A9A9", alpha = 0.3) +
  
  # Forecast Line
  geom_line(color = "#3A3A66", size = 1.2, linetype = "solid") +
  # Forecast Points (Slightly smaller, matching line color)
  
  # update the x axis with quarters
  scale_x_continuous(
    # Use the calculated numeric quarters for breaks
    breaks = forecast_quarters_numeric,
    # Use the calculated Year-Quarter strings for labels
    labels = forecast_quarters_labels) +
    labs(title = "ECB Deposit Facility Rate Forecast",
         subtitle = paste("Model: Taylor Rule with", model_name, "| Confidence Bands based on Standard Error"),
         caption = "Inner band: ±1 S.D. (approx. 68%); Outer band: ±2 S.D. (approx. 95%)",
         y = "Interest Rate (%)", x = "") +
  theme_light() +
  theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
        plot.subtitle = element_text(size = 12, color = "#555555", hjust = 0.5),
        axis.title.x = element_text(size = 12, margin = margin(t=10)),
        axis.title.y = element_text(size = 12, margin = margin(r=10)),
        plot.caption = element_text(size = 9, color = "grey50", hjust = 0),
        panel.grid.major.x = element_blank(), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 10))
final_fc_plot2

```
















