---
title: "ECB Project - Coding Appendix"
output:
  pdf_document: 
    toc: true
    fig_caption: yes
    toc_depth: 3
    latex_engine: lualatex
    keep_tex: false
header-includes:
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \usepackage[backend=biber,style=authoryear]{biblatex}  
  - \addbibresource{citations.bib}  # BibTeX file
---
\newpage

# Preliminaries

## Setup
```{r setup, results=FALSE, warning=FALSE, message=FALSE, echo=TRUE}

# 1. Clear memory
rm(list=ls())


# 2. Load here package to enable finding script loading other packages 

# First, define a function that checks if a package is installed. 
# If not, it installs it. Then it loads it.
package_loader <- function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    message(paste("Installing missing package:", pkg))
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}
package_loader("here")


# 3. Load packages & helper functions for plots and tables
source(here("scripts/packages_script.R"))
source(here("helpers/raw_plotter.R"))
source(here("helpers/stationarity_tables.R"))
source(here("helpers/taylor_tables.R"))
source(here("helpers/struct_breaks_tables.R"))
source(here("helpers/roll_TR_plotter.R"))
source(here("helpers/pseudo_outofsample_tables.R"))
source(here("helpers/pseudo_outofsample_plots.R"))
source(here("helpers/actual_forecast_displays.R"))


# 4. Load API keys for data (only FRED required)
fredr_set_key(Sys.getenv("FRED_API_KEY"))


# 5. Dates (to automatically get the latest data from API calls
start_date <- "1999-01-01"
end_date <- Sys.Date()

```

## Interactive Option Selection

* Use Hamilton Filter: 
  + TRUE: Selects Hamilton method for output gap estimation
  + FALSE: Selects Hodrick-Prescott method for output gap estimation

* Use Inflation Expectations:
  + TRUE: The models used for forecasting will use 12-month ahead inflation
          expectations from the ECB survey of professional forecasts (average).
  + FALSE: The models used for forecasting will use realised inflation
  
* Use Formula
  + Formula 1: Actual interest rate regressed on inflation and output gaps  
  + Formula 2: Shadow interest rate regressed on inflation and output gaps
  + Formula 3: Actual interest rate regressed on the one-quarter lag of the interest
                rate and on inflation and output gaps
  + Formula 4: Shadow interest rate regressed on the one-quarter lag of the shadow 
                interest rate and on inflation and output gaps
                
* Format:
  + html: For outputting in console or knitting to html
  + latex: For knitting to pdf

```{r options, results=FALSE, warning=FALSE, message=FALSE, echo=TRUE}

#paste options config here and in main then delete the script file
source(here("scripts/options_config_script.R"))

```

```{r taylor-rule-formulas, echo=TRUE, message=FALSE, warning=FALSE}

#full paste
source(here("scripts/taylor_rule_formulas_script.R"))

```

```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```

# Data

## Sources \& Explanations

* FRED Data Source: European Central Bank, ECB Deposit Facility Rate for Euro 
                    Area [ECBDFR], retrieved from FRED, Federal Reserve Bank of 
                    St. Louis; https://fred.stlouisfed.org/series/ECBDFR. 
                    The data is the Deposit Facility and is directly reprinted 
                    from the ECB. It's in percent and not seasonally adjusted. 
                    The ECB Monetary Policy is steered through this rate.

* Shadow Interest Rate: The Shadow rate was developed by 
                        \cite{wuTimeVaryingLowerBound2017} and does quantify 
                        a hypothetical removal of the ZLB. The rate does not 
                        track 1:1 on the deposit facility in the data before 
                        the ZLB, instead being closer to the refinancing rate. 
                        \textcolor{red}{(maybe need to adjust, oui je crois faux, surtout la                          partie sur refinancing rate)}

* GDP: The GDP Data is quarterly real GDP in 2010 Euros in million retrieved from FRED 
       via Eurostat. The data is seasonally adjusted.

* Potential GDP: Either estimated with the Hodrick-Prescott filter or the Hamilton 
filter, based on the formulas described in equation \eqref{eq:filters}:

\begin{equation} 
\begin{aligned}
  \text{HP Filter:} \quad & \min_{\tau} \left( \sum_{t=1}^{T} (y_t - \tau_t)^2 + \lambda \sum_{t=2}^{T-1} \left[ (\tau_{t+1} - \tau_t) - (\tau_t - \tau_{t-1}) \right]^2 \right) \\
  \text{Hamilton:} \quad & y_t = \beta_0 + \sum_{j=1}^{p} \beta_j y_{t-h-j+1} + v_t \quad \text{(where } v_t \text{ is the cycle)}
\end{aligned}
\label{eq:filters}
\end{equation}

* Inflation:
  + Realised: The Inflation data is from Eurostat and measures HICP monthly data (annual rate of change). It's the index that the ECB uses for Inflation and is not seasonally adjusted, but the 
    fact that it represents the year-on-year change in prices implies there is no seasonality.
  + Expectations: Expected Inflation is the ECB's survey of professional 
    forecasters. It tracks professional forecasts of HICP inflation 12 months in advance.


## Loading \& Preparation Data 
To get this data, we run the following code using API calls to FRED, EuroStat, and
DBnomics. We convert each variable to the quarterly average and then store them all
to one dataframe named "data". It includes a date column, the deposit rate, the shadow
rate, inflation, inflation gap (which is just inflation minus the ECB target, which is
set at 2\%), expected inflation, expected inflation gap, real GDP, GDP output gap 
computed with the HP filter and GDP output gap computed with the Hamilton filter.
```{r data, echo=TRUE, message=FALSE, warning=FALSE}

#copypaste data script and explain
source(here("scripts/data_script.R"))

```

## Options Configuration
We then implement the chosen options on using inflation expectations or not, and
on using either the HP filter or the Hamilton filter for output gap.
```{r data_config, message=TRUE, warning=FALSE, results='asis'}

#paste
source(here("scripts/options_implement_script.R"))

```

## Raw Data Plots
```{r data_plots, message=FALSE, warning=FALSE}

# Who cares about the method here, only result matters, paste
source(here("scripts/raw_data_plot_script.R"))

```

## Data Properties
In this section, we run various tests to understand the properties of our main variables,
interest rate, inflation and output gap.
These include both ADF and KPSS stationarity tests (using the tseries package) and a cointegration test (using the aTSA package) to see if the interest rate is 
cointegrated with inflation. We wrap these in custom functions that automatically report
the results and intepret them based on the relevant p-value.
```{r stat_tests_func, echo=TRUE, message=FALSE, warning=FALSE}

# copy paste both stat and coint funcs and explain
source(here("helpers/stationarity_tests.R"))

```


```{r data_prop, echo=TRUE, message=FALSE, warning=FALSE}

source(here("scripts/data_properties_script.R"))

```


```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```

# Taylor Rule Estimation
We now run basic Taylor Rule estimations using OLS on the complete dataset in order
to get some information on the historical Taylor coefficients.

## Without Lags
The "No Lag" Taylor Rule specifications follow the regression expressed in equation \eqref{eq:nolagTR}. We run 6 variations total of this, starting with the base case
using the actual deposit rate, realised inflation (gap) and the output gap. We then 
mix cases where we use the shadow rate, expected inflation (gap) and finally including
both expected and realised inflation (gap).

\begin{equation} 
\begin{aligned}
  i_t &= \pi^* + \beta(\pi_t-\pi^*) + \gamma(y_t-\bar{y_t}) 
\end{aligned}
\label{eq:nolagTR}
\end{equation}

```{r Taylor Rules w/o lag, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste first half of this script
source(here("scripts/taylor_estim_no_lag_script.R"))

```

```{r Taylor Rules w/o lag but with inflation expectations, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste second half of this script
source(here("scripts/taylor_estim_no_lag_script.R"))

```

## Lagged Models 
The lagged Taylor Rule specifications follow the regression expressed in equation \eqref{eq:lagTR} which is the same as \eqref{eq:nolagTR} but also including a 
one-quarter lag for the interest rate. We then run the same 6 variations as for 
the models without the lag.

\begin{equation} 
\begin{aligned}
  i_t &= \pi^* + \phi i_{t-1} + \beta (\pi_t-\pi^*) + \gamma(y_t-\bar{y_t})
\end{aligned}
\label{eq:lagTR}
\end{equation}

```{r Taylor Rules w/ lag, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste first half of this script
source(here("scripts/taylor_estim_with_lag_script.R"))

```

```{r Taylor Rules w/ lag but with inflation expectations, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste second half of this script
source(here("scripts/taylor_estim_with_lag_script.R"))

```

## Checking for structural breaks
The previous Taylor Rule estimation are most likely flawed if there have been 
structural breaks in the economic system that would influence the way the ECB reacts
to the economy. Given this, we run both a Chow test and a Bai-Perron test in order
to see if there are any using the "strucchange" package. The Chow test is run on 
two dates where we suspect there have been breaks which are in the third quarter of 
2012, representing the start 
of the Zero Lower Bound, and in the first quarter of 2020, representing the start
of the COVID-19 pandemic. The Bai-Perron test, however, works by finding any
structural breaks present in the overall sample \textcolor{red}{which minimise
the Aikake Information Criterion}. 

```{r chow test func, echo=TRUE, message=FALSE, warning=FALSE}

#only paste chow ones 
source(here("helpers/struct_breaks_tests.R"))

```

```{r chowtest, echo=TRUE, message=FALSE, warning=FALSE}

#only paste chow ones 
source(here("scripts/struct_breaks_tests_script.R"))

```

```{r bai-perron test func, echo=TRUE, message=FALSE, warning=FALSE}

#only paste bp ones
source(here("helpers/struct_breaks_tests.R"))

```

```{r bai-perron test, echo=TRUE, message=FALSE, warning=FALSE}

#only paste bp ones
source(here("scripts/struct_breaks_tests_script.R"))

```



## Rolling Estimation 
Given that there have been structural breaks (according to our tests), we decide 
to run a rolling estimation scheme for the Taylor Rule formula selected in the 
setup options configuration. To do this, we use a custom function which runs the 
chosen model firstly on the first data "window" (which can be selected to be 
however many quarters desired) and then moves this window forward in time by one
quarter at a time, running the estimation for each loop. We then use another
custom function to plot the resulting coefficients through time, including 
confidence intervals.

```{r rolling TR func, message=FALSE, warning=FALSE}

source(here("helpers/roll_TR_estimator.R"))

```

```{r rolling TR, message=FALSE, warning=FALSE}

source(here("scripts/roll_TR_script.R"))

```


```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```


# Forecasting Model Evaluation

## Methodology
Our method of forecasting consists of two steps. First, we run our custom auto 
ARIMA function on the inputs of the Taylor Rule (inflation and output gap). This 
function replicates the one in the "forecast" package, though does not loop over
the differentiation parameter as we want to set that value manually based on the 
stationarity tests run above. We additionally run this function on the interest
rate in order to serve as our benchmark model for relative forecasting performance 
evaluation. In order to compute the forecasts based on the fitted ARIMA models,
we use a custom function that replicates the same use-case in the "forecast" package.
At the same time as the ARIMA fits, we use OLS to compute the Taylor Rule coefficients.
The second step is then to do the actual forecasting, where we predict future values
of the interest rate based on the forecasted values of the inputs using the fitted
ARIMA models, weighted by the coefficients computed with OLS. 

```{r auto_arima_func, echo=TRUE, message=FALSE, warning=FALSE}

source(here("helpers/auto_ARIMA_replic.R"))

```


## Pseudo Out of Sample Estimation
In order to asses the performance of this methodology, we use a pseudo out-of-sample
rolling estimation scheme of direct forecasts for all Taylor Rule formulas and for
the benchmark ARIMA. We forecast up to a horizon of 10 quarters ahead. 
To do this, we loop over quarters starting just before the evaluation sample,
where we then implement our methodology to forecast up to the farthest horizon.
We then store these values and the loop starts again, adding one more quarter from
the evaluation sample, and dropping the very first quarter in the overall sample.
For the lagged models, we additionally need to include an iterative forecasting step, where 
we loop over each horizon in order to use the previous point forecast to compute
the next one. In order to speed up the process, we include a simple parallelization
procedure which runs these loops in parallel on a fourth the the users' CPU cores.

```{r pseudo-out-of-sample-parameters, echo=TRUE, message=FALSE, warning=FALSE}

#paste just step 0 
source(here("scripts/pseudo_out_of_sample_estimation_script.R"))

```

```{r pseudo-out-of-sample-estimation, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste steps 1-end
source(here("scripts/pseudo_out_of_sample_estimation_script.R"))

```

## Spaghetti Plots
In order to visualize the pseudo out-of-sample evaluation exercise, we use spaghetti
plots of the point forecasts which allow to compare how often the model "misses" the 
realised values of the interest rate between different horizons and dates. We do
the plot only on the on the Taylor Rule formula selected in the options configuration
in order to avoid clutter.

```{r Spaghetti Plot, message=FALSE, warning=FALSE}

#needs to run but dont paste anything
source(here("helpers/pseudo_outofsample_tests.R"))

#paste only first
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

## Forecast Errors 

### Plots of FE
Similar to the spaghetti plot of the point forecasts, we essentially use the same 
plot but computing the forecast errors as the difference between our point
forecast and the realised value.
```{r Plot of Forecast Errors, warning=FALSE, message=FALSE}

#paste only second
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

### Density of FE
We can then plot the distribution of these forecast errors using a density plot.
We include 3 versions: one that is unscaled in order to compare the absolute values,
one that is scaled in order to compare relative differences, and one that plots
density in a way to compare both.
```{r Density of Forecast Errors}

#paste only third
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

### Variance of FE
```{r Variance of Forecast Errors}

#paste only fourth
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

### Horizon 1 Autocorrelation of FE
\textcolor{red}{Model 1, 2, 3 are autocorrelated at h=1. Include results in paper,
not here in the appendix. Well maybe not idk.}

```{r Durbin Watson func, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only fourth
source(here("helpers/pseudo_outofsample_tests.R"))

```

```{r Durbin Watson test, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only fifth
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```


### Overall Autocorrelation of FE
\textcolor{red}{We don't reject H0 -> h+1 are uncorrelated
1,2, are autocorrelated, 3 and 4 are not.}

```{r Ljung Box func, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only fifth
source(here("helpers/pseudo_outofsample_tests.R"))

```

```{r Ljung Box test, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only sixth
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

### Errors Normally Distributed
Run the Jarque-Bera test using a custom function that \textcolor{red}{explain loop
and package where the actual test function is made}

```{r jb_test_func, echo=TRUE, message=TRUE, warning=FALSE}

#print only third
source(here("helpers/pseudo_outofsample_tests.R"))

```

```{r Jarque Bera Test, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only seventh
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```




## Absolute Performance: Efficiency \& Bias
In order to evaluate the absolute performance of our model with the results
from the pseudo out-of-sample estimation, we use Mincer-Zarnowitz regressions
to check for both efficiency and bias. The method is to regress the actual realised
values of the interest rate on the forecasted ones, as seen in equation 
\eqref{eq:MZ}. We then run a joint test, on the null 
hypothesis that $\alpha = 0$ (model is unbiased) and $\beta = 1$ (model is efficient).
We run this with a custom function that loops over each horizon, in order to assess
the performance of our models in the shorter vs longer term, and do so on all
Taylor Rule formulas as well as on the benchmark

\begin{equation} 
\begin{aligned}
  i_t^{actual} = \alpha + \beta i_t^{forecast} + \epsilon_t
\end{aligned}
\label{eq:MZ}
\end{equation}

```{r mz_test_func, echo=TRUE, message=TRUE, warning=FALSE}

# in final markdown, input just the relevant code, i.e. the tests made
# who cares about code to create tables, that stays in helpers
source(here("helpers/pseudo_outofsample_tests.R"))

```


```{r forecast-model-efficiency, echo=TRUE, message=TRUE, warning=FALSE, results='asis'}

#paste only eighth
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```


## Relative Performance (against benchmark)
In order to evaluate the relative performance of our model against the benchmark
ARIMA, we first compute the Mean Squared Forecast Errors (MSFE) for each horizon. 
This allows us to see the average performance of each estimation. For a deeper
comparison, we implement Diebold-Mariano tests on each horizon. This test compares 
the forecast errors of both models by computing a loss differential, which is the 
difference of the squared forecast errors between the main model and the benchmark.
The test statistic is as reported in equation \eqref{eq:DM} \textcolor{red}{and 
assumes that $\hat{d_{h,t}}$ is covariance stationary}. We include three versions
of the test, one where the alternative hypothesis is that the two models bring
about different losses, one where the alternative hypothesis is that the tested 
model is greater than the benchmark, and one where the alternative hypothesis is
that the tested model is worse than the benchmark (greater loss). The null 
hypothesis is the same for all three, being that the two models bring about the 
same loss.


\begin{equation} 
\begin{aligned}
  DM &= \frac{\bar{\hat{d_h}}}{\hat{\sigma^2_{\bar{\hat{d_h}}}}} \\
   &= \frac{\frac{1}{P}\sum^{T-h}_{t=R}\hat{d}_{h,t}}{\hat{\sigma^2_{\bar{\hat{d_h}}}}} \\
   \text{where } \hat{d_{h,t}} &= \hat{e_{1,t+h|t}^2} - \hat{e_{2,t+h|t}^2}
\end{aligned}
\label{eq:DM}
\end{equation}

```{r dm_test_func, echo=TRUE, message=TRUE, warning=FALSE}

# in final markdown, input just the relevant code, i.e. the tests made
# who cares about code to create tables, that stays in helpers
source(here("helpers/pseudo_outofsample_tests.R"))

```

```{r forecast-model-perf, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#paste only ninth
source(here("scripts/pseudo_out_of_sample_results_script.R"))

```

```{r, echo=FALSE, results='asis'}
cat("\\clearpage")
```



# Actual Forecast Model
As described earlier, our methodology consists of two steps where we first
fit ARIMA models to the Taylor Rule inputs as well as estimate the coefficients,
and then use the forecasted inputs and the coefficients to compute our point
forecast for the interest rate. In this section however, we run this method on
the entire sample (Q1 1999 to Q3 2025) making forecasts up to 10 quarters ahead
(Q1 2028).

## Forecasting
In order to do this, we use a custom function that first fits the ARIMA models
for the inputs, uses these to make forecasts of said inputs, estimates the Taylor
Rule coefficients and then computes the point forecast for the interest rate.
Note that for the lagged models, we then use these point forecasts to compute the
next forecast iteratively. It also fits an ARIMA model for the interest rate,
so that we have a simple benchmark model to see if there are large differences
in estimates.

```{r forecast funcs, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#do paste without pred int part
source(here("helpers/actual_forecast_estimator.R"))
 
```

```{r forecast, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#do paste without pred int part
source(here("scripts/final_forecast_script.R"))
 
```

## Prediction Intervals
To compute the prediction intervals for our point forecasts, we use the estimated
variance of errors computed in the pseudo out-of-sample evaluation exercise 
assuming that the distribution of errors will be the same for our actual future
forecast. We include two intervals, one being one standard deviation away from the
point forecasts, and the other being two standard deviations away. This is computed
as in equation \eqref{eq:PI}.

\begin{equation} 
\begin{aligned}
  PI_1^{upper} &= i_t^{forecast} + \sigma_{e} \\
  PI_1^{lower} &= i_t^{forecast} - \sigma_{e} \\
  PI_2^{upper} &= i_t^{forecast} + 2\sigma_{e} \\
  PI_2^{lower} &= i_t^{forecast} - 2\sigma_{e} 
\end{aligned}
\label{eq:PI}
\end{equation}


```{r prediction_intervals func, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#do paste only pred int part
source(here("helpers/actual_forecast_estimator.R"))

```


```{r prediction_intervals, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

#do paste only pred int part (4 and 5)
source(here("scripts/final_forecast_script.R"))

```








